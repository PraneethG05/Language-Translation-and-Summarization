{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a03893-60f1-4070-b1bc-80e7d7a0dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092dc96a-1c1c-44f2-9adc-1fb00355044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"opus100\",'en-hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e648e070-5d5d-4fa3-91c2-0a03735b59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def save_translation_to_csv(input_dict, output_file):\n",
    "    # Extract Hindi and English texts from the input dictionary\n",
    "    hindi_text = input_dict['translation']['hi']  # Extract Hindi text\n",
    "    english_text = input_dict['translation']['en']  # Extract English text\n",
    "\n",
    "    # Create a list of dictionaries containing the data to be saved in the CSV file\n",
    "    data = [{'Hindi': hindi_text, 'English': english_text}]\n",
    "\n",
    "    # Define the CSV file headers\n",
    "    fieldnames = ['Hindi', 'English']\n",
    "\n",
    "    # Check if the CSV file already exists\n",
    "    file_exists = os.path.isfile(output_file)\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header only if the file is newly created\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the data row\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a46d9a-e172-4fef-87b3-00e24730a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_file = 'translation.csv'\n",
    "for n in range(6000):\n",
    "   input_text=dataset['train'][n]\n",
    "   save_translation_to_csv(input_text, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8cc14c-d25c-4f82-bc85-562fe6e5cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de5be304-90e3-42fe-8cee-a818986a8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f7667f-0209-41bf-a90d-2147c34042b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file1 = 'translation.csv'\n",
    "dataset1 = pd.read_csv(dataset_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e42425e-e43c-4bb9-8b75-1ef463f23f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>शराबी</td>\n",
       "      <td>Sharaabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- जॉन कॉलिन्स</td>\n",
       "      <td>- John Collins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47995</th>\n",
       "      <td>ब्राउजिंग इस्तेमाल करें</td>\n",
       "      <td>Use browsing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>उत्तरपश्चिम</td>\n",
       "      <td>northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47997</th>\n",
       "      <td>चैनल्स</td>\n",
       "      <td>Channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>तो तुम दोनों अपने परवरदिगार की किन किन नेअमतों...</td>\n",
       "      <td>So which of the favors of your Lord would you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>SSL के ऊपर IMAP</td>\n",
       "      <td>IMAP over SSL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Hindi  \\\n",
       "0                                                  शराबी   \n",
       "1      राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...   \n",
       "2      मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...   \n",
       "3       यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।   \n",
       "4                                          - जॉन कॉलिन्स   \n",
       "...                                                  ...   \n",
       "47995                            ब्राउजिंग इस्तेमाल करें   \n",
       "47996                                        उत्तरपश्चिम   \n",
       "47997                                             चैनल्स   \n",
       "47998  तो तुम दोनों अपने परवरदिगार की किन किन नेअमतों...   \n",
       "47999                                    SSL के ऊपर IMAP   \n",
       "\n",
       "                                                 English  \n",
       "0                                               Sharaabi  \n",
       "1      politicians do not have permission to do what ...  \n",
       "2             I'd like to tell you about one such child,  \n",
       "3      This percentage is even greater than the perce...  \n",
       "4                                         - John Collins  \n",
       "...                                                  ...  \n",
       "47995                                       Use browsing  \n",
       "47996                                          northwest  \n",
       "47997                                           Channels  \n",
       "47998  So which of the favors of your Lord would you ...  \n",
       "47999                                      IMAP over SSL  \n",
       "\n",
       "[48000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e2667d-557c-4c1f-9738-fc0c7f00d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=dataset1[:5000]\n",
    "val_dataset=dataset1[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b7c477-e7dc-4f80-9152-fc058d29f6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3772b8a-075a-4239-808a-76b6d8b47194",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=train_dataset[:1000]\n",
    "data2=train_dataset[1000:2000]\n",
    "data3=train_dataset[2000:3000]\n",
    "data4=train_dataset[3000:4000]\n",
    "data5=train_dataset[4000:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b3fd53-b0d4-4c54-84af-1db6142fadef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hinditext', 'englishtext'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Creating individual training datasets\n",
    "train_dataset1 = pd.DataFrame()\n",
    "train_dataset1['hinditext'] = 'Can you translate \"' + data1['Hindi'] + '\" into English?'\n",
    "train_dataset1['englishtext'] = 'Yes, sure. Here is the English translation: \"' + data1['English'] + '\".'\n",
    "\n",
    "train_dataset2 = pd.DataFrame()\n",
    "train_dataset2['hinditext'] = 'Please translate \"' + data2['Hindi'] + '\" into English.'\n",
    "train_dataset2['englishtext'] = 'Okay, the translation into English is: \"' + data2['English'] + '\". I hope this helps.'\n",
    "\n",
    "train_dataset3 = pd.DataFrame()\n",
    "train_dataset3['hinditext'] = 'Can you translate \"' + data3['Hindi'] + '\" into English?'\n",
    "train_dataset3['englishtext'] = \"Yeah, sure, I can help with that. Here is the translation into English: \" + data3['English'] + '.'\n",
    "\n",
    "train_dataset4 = pd.DataFrame()\n",
    "train_dataset4['hinditext'] = 'Translate Hindi to English. Here is the text: \"' + data4['Hindi'] + '\".'\n",
    "train_dataset4['englishtext'] = 'Sure, the translation of your text into English is: \"' + data4['English'] + '\".'\n",
    "\n",
    "train_dataset5 = pd.DataFrame()\n",
    "train_dataset5['hinditext'] = 'Translate Hindi text to English. The text is: \"' + data5['Hindi'] + '\".'\n",
    "train_dataset5['englishtext'] = 'Sure, here is the English translation of your text: \"' + data5['English'] + '\".'\n",
    "\n",
    "# Appending all datasets into a single DataFrame\n",
    "appended_dataframe = pd.concat([train_dataset1, train_dataset2, train_dataset3, train_dataset4, train_dataset5], ignore_index=True)\n",
    "\n",
    "# Converting to Hugging Face Dataset\n",
    "train_data = Dataset.from_pandas(appended_dataframe)\n",
    "\n",
    "# Display the dataset\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94febb85-14ef-40fc-9edb-5d11548d2bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate Hindi to English. Here is the text: \"और पांच गोबोट. और यही उसका खजाना था.\".'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['hinditext'][3002]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb54bca1-9fcd-4ecd-8e40-1ab2d4637692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, the translation of your text into English is: \"and five Gobots. And this was his treasure.\".'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['englishtext'][3002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca79491-fa63-49e1-9de1-18c39d4f9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=val_dataset[:200]\n",
    "data2=val_dataset[200:400]\n",
    "data3=val_dataset[400:600]\n",
    "data4=val_dataset[600:800]\n",
    "data5=val_dataset[800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76093578-d254-4ac8-9c21-86955586e09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hinditext', 'englishtext'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create validation datasets with Hindi-to-English prompts and translations\n",
    "val_dataset1 = pd.DataFrame()\n",
    "val_dataset1['hinditext'] = 'Can you translate \"' + data1['Hindi'] + '\" into English?'\n",
    "val_dataset1['englishtext'] = 'Yes sure, here is the English translation: \"' + data1['English'] + '\".'\n",
    "\n",
    "val_dataset2 = pd.DataFrame()\n",
    "val_dataset2['hinditext'] = 'Please translate \"' + data2['Hindi'] + '\" to English.'\n",
    "val_dataset2['englishtext'] = 'Okay, the translation into English is: \"' + data2['English'] + '\". I hope this helps.'\n",
    "\n",
    "val_dataset3 = pd.DataFrame()\n",
    "val_dataset3['hinditext'] = 'Please translate \"' + data3['Hindi'] + '\" to English.'\n",
    "val_dataset3['englishtext'] = \"Yeah sure, I can help you with that. Here is the translation into English: \" + data3['English'] + '.'\n",
    "\n",
    "val_dataset4 = pd.DataFrame()\n",
    "val_dataset4['hinditext'] = 'Can you translate \"' + data4['Hindi'] + '\" into English?'\n",
    "val_dataset4['englishtext'] = 'Yes, the translation of \"' + data4['Hindi'] + '\" to English is: \"' + data4['English'] + '\".'\n",
    "\n",
    "val_dataset5 = pd.DataFrame()\n",
    "val_dataset5['hinditext'] = 'Translate Hindi text to English. Here is the text: \"' + data5['Hindi'] + '\".'\n",
    "val_dataset5['englishtext'] = 'Sure, here is the English translation of your text: \"' + data5['English'] + '\".'\n",
    "\n",
    "# Combine all validation datasets into a single DataFrame\n",
    "appended_dataframe = pd.concat([val_dataset1, val_dataset2, val_dataset3, val_dataset4, val_dataset5], ignore_index=True)\n",
    "\n",
    "# Convert the DataFrame into a Hugging Face Dataset\n",
    "val_data = Dataset.from_pandas(appended_dataframe)\n",
    "\n",
    "# Display the dataset\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6001cc3d-46bf-4ea8-871b-f54059f9de6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate Hindi text to English. Here is the text: \"इसके बाद व्यास जी ने २४००० श्लोकों का बिना किसी अन्य ऋषियों चन्द्रवंशी-सूर्यवंशी राजाओं के उपाख्यानों का केवल भारतवंशियों को केन्द्रित करके भारत काव्य बनाया।\".'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['hinditext'][902]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b338d2ae-ffe4-4d50-8768-05105884624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here is the English translation of your text: \"Afterwards he composed a 25,000 verse poem, without referencing (focusing) on the various religious doctrines, kings and great sages and created the poem “”Bharata“”.\".'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['englishtext'][902]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a3279f3-aa37-4bad-b6cc-4280833a2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,MBartForConditionalGeneration\n",
    "\n",
    "model_ckpt = \"facebook/mbart-large-50\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "668d99e3-2a91-49cc-9237-4b1528732529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf7c878586d404fba0dd159b6fc669f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b728925ca0404e22acf6cec9601bdf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    # Tokenize the input Hindi text\n",
    "    input_encodings = tokenizer(\n",
    "        example_batch[\"hinditext\"], \n",
    "        max_length=1024, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the target English text\n",
    "    target_encodings = tokenizer(\n",
    "        example_batch[\"englishtext\"], \n",
    "        max_length=1024, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Convert training dataset\n",
    "train_dataset_tf = train_data.map(\n",
    "    convert_examples_to_features, \n",
    "    batched=True, \n",
    "    remove_columns=[\"hinditext\", \"englishtext\"]\n",
    ")\n",
    "\n",
    "# Convert validation dataset\n",
    "val_dataset_tf = val_data.map(\n",
    "    convert_examples_to_features, \n",
    "    batched=True, \n",
    "    remove_columns=[\"hinditext\", \"englishtext\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4aef6e3-20b7-48ba-8aa0-b304849fb5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df5b0056-363d-44ec-b1a3-abe0aa1018eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b70776e8-03b4-479e-b3ef-d290eaf70885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='mbartTrans', \n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy='steps',  # Updated from evaluation_strategy to eval_strategy\n",
    "    save_strategy='no', \n",
    "    eval_steps=2000,\n",
    "    logging_steps=1000,\n",
    "    weight_decay=0.01, \n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adafactor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7e6bb15-4719-40d1-990a-0bef988a0a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0895ae95-c41f-45cc-8d24-ab86b1c88159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1245/3250986335.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(model=model, args=training_args, tokenizer=tokenizer,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:06:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.084537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.089467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.092005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.090944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.090545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=0.13896226806640624, metrics={'train_runtime': 4016.3143, 'train_samples_per_second': 2.49, 'train_steps_per_second': 2.49, 'total_flos': 2.167129767936e+16, 'train_loss': 0.13896226806640624, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(model=model, args=training_args, tokenizer=tokenizer,\n",
    "                  data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=train_dataset_tf,\n",
    "                        eval_dataset=val_dataset_tf)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3664be9a-a4be-40c7-baf7-055b18c39c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"varshith10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fd5869-8e99-4b61-8fb7-6305869cb824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/mbartfortranslation.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('mbartfortranslation', 'zip', 'varsith10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad1428a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define local model directory and target directory on Google Drive\n",
    "local_model_dir = \"varshith10000\"\n",
    "drive_model_dir = \"/content/drive/My Drive/varshith10000\"\n",
    "\n",
    "# Copy the saved model to Google Drive\n",
    "shutil.copytree(local_model_dir, drive_model_dir)\n",
    "\n",
    "print(f\"Model has been uploaded to {drive_model_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1dc26eb-f516-40ec-9534-68a9d1fafad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, AutoTokenizer,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c982f2-6ada-47b5-9e65-414e7dab03c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Text: Translate the following Hindi text to English: '1800 फर्जी बैंक खाते खोल देशभर में बेचे, वसूली 2 करोड़ रुपये की रकम, फर्जीवाड़े की इनसाइड स्टोरी'\n",
      "Translated English Text: Sure, here is the English translation of your text: '1800 fraudulent bank accounts opened in all over the country, fined Rs. 2 crores, the indictment of the scammer.'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_ckpt = \"varshith10000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to('cuda')  # Use 'cuda' if you're using GPU, else 'cpu'\n",
    "\n",
    "# Prepare the input prompt (in Hindi)\n",
    "input_text = \"Translate the following Hindi text to English: '1800 फर्जी बैंक खाते खोल देशभर में बेचे, वसूली 2 करोड़ रुपये की रकम, फर्जीवाड़े की इनसाइड स्टोरी'\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_encodings = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate the translation\n",
    "with torch.no_grad():\n",
    "    translated_ids = model.generate(input_encodings['input_ids'].to('cuda'), num_beams=5, max_length=50, early_stopping=True)\n",
    "\n",
    "# Decode the generated IDs back to text (English)\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original Hindi Text: {input_text}\")\n",
    "print(f\"Translated English Text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f098321-57b9-43f1-958d-9470471aa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "पुलिस के मुताबिक, इस गिरोह का मास्टर माइंड शशिकातं कुमार है. यह शख्स बिहार के नालंदा का रहने वाला है. इसने 12वीं तक पढ़ाई की है. यह लोगों के फर्जी पैन कार्ड बनाता था. शशिकांत ही वह शख्स है, जो झारखंड से डाटा लाता था. उसके बाद ये पता करता था कि किस आधार कार्ड से पैन कार्ड नहीं बना है. जिस आधार का पैन कार्ड नहीं होता था, उसके लिए वह ऑफलाइन आवेदन करता था. इसके बाद वह इन कागजात के आधार पर सिम लेता था और फिर बैंक खाते खुलवाता था. जो भी सदस्य बैंक खाता खुलवाता, उसे उसके लिए दो हजार रुपये कमीशन मिलता था."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6324e0-d113-496a-a596-be9725bfdbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Text: Translate the following Hindi text to English: 'पुलिस के मुताबिक, इस गिरोह का मास्टर माइंड शशिकातं कुमार है. यह शख्स बिहार के नालंदा का रहने वाला है. इसने 12वीं तक पढ़ाई की है. यह लोगों के फर्जी पैन कार्ड बनाता था. शशिकांत ही वह शख्स है, जो झारखंड से डाटा लाता था. उसके बाद ये पता करता था कि किस आधार कार्ड से पैन कार्ड नहीं बना है. जिस आधार का पैन कार्ड नहीं होता था, उसके लिए वह ऑफलाइन आवेदन करता था. इसके बाद वह इन कागजात के आधार पर सिम लेता था और फिर बैंक खाते खुलवाता था. जो भी सदस्य बैंक खाता खुलवाता, उसे उसके लिए दो हजार रुपये कमीशन मिलता था.'\n",
      "Translated English Text: Sure, here is the English translation of your text: 'Police says, the mastermind of this group is Shafikhata Kumar, a native of Bihar. He went to 12th and made a fake passport. Shafikhata Kumar is the person who first came to collect data from Jharkhand. He then determined which is not a valid passport. The person who had a valid passport, the person who had a valid passport, the person who had a valid passport, the person who had a valid bank account, the person who had a valid bank account, the person who had a valid bank account.\".\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_ckpt = \"varshith10000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to('cuda')  # Use 'cuda' if you're using GPU, else 'cpu'\n",
    "\n",
    "# Prepare the input prompt (in Hindi)\n",
    "input_text = \"Translate the following Hindi text to English: 'पुलिस के मुताबिक, इस गिरोह का मास्टर माइंड शशिकातं कुमार है. यह शख्स बिहार के नालंदा का रहने वाला है. इसने 12वीं तक पढ़ाई की है. यह लोगों के फर्जी पैन कार्ड बनाता था. शशिकांत ही वह शख्स है, जो झारखंड से डाटा लाता था. उसके बाद ये पता करता था कि किस आधार कार्ड से पैन कार्ड नहीं बना है. जिस आधार का पैन कार्ड नहीं होता था, उसके लिए वह ऑफलाइन आवेदन करता था. इसके बाद वह इन कागजात के आधार पर सिम लेता था और फिर बैंक खाते खुलवाता था. जो भी सदस्य बैंक खाता खुलवाता, उसे उसके लिए दो हजार रुपये कमीशन मिलता था.'\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_encodings = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate the translation\n",
    "with torch.no_grad():\n",
    "    translated_ids = model.generate(input_encodings['input_ids'].to('cuda'), num_beams=5, max_length=1000, early_stopping=True)\n",
    "\n",
    "# Decode the generated IDs back to text (English)\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original Hindi Text: {input_text}\")\n",
    "print(f\"Translated English Text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461c0e4-ad9a-4f33-90f1-dcb0e6bf01c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
